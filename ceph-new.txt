ceph

实验准备： 一台客户机，三台存储集群虚拟机

一、实验环境：
1、配置yum源  ，物理机通过 ftp 发布
创建文件：mkdir   /var/ftp/ceph
自定义yum源rpm包解压到指定文件： tar   -xf    xxxxx.gz    -C   /var/ftp/ceph
重启  vsftpd  服务
在一个台机器配置yum，传送给每一台机， 查看yum清单 repodata 在哪个文件夹，就写哪个，这里是在MON，OSD，Tools 所以如下：
cat  /etc/yum.repo.d/ceph.repo
[ mon ]
name=
baseurl=ftp://xxxxxx/ceph/xxxxx/MON
gpgcheck=0
[ osd ]
name=
baseurl=ftp://xxxxxx/ceph/xxxxx/OSD
gpgcheck=0
[ tools ]
name=
baseurl=ftp://xxxxxx/ceph/xxxxx/Tools
gpgcheck=0

2、修改  /etc/hosts 并同步到所有主机 ： 后面实验不用打ip  ，直接打主机名就好了

3、配置无密码连接

4、配置NTP时间同步：三台存储集群虚拟机服务器指向客户端
client  ：   server 0.cemtos.pool.ntp.org   iburst
                 allow   192.168.4.0/24
                 local   startum  10
node1~3 :   server 192.168.4.10    iburst
重启chrony服务

5、准备存储磁盘，为每台存储机加三块10G的硬盘（一块存储日志信息，两块作为OSD存储设备）


二、部署ceph集群

1、安装 ceph-deploy  工具：yum -y  install  ceph-deploy

2、创建目录并进入该目录下：mkdir  ceph-cluster      cd   ceph-cluster/

3、创建ceph集群配置  ceph-deploy   new   node1  node2   node3 

4、给所有节点安装软件包  ceph-deploy  install  node1   node2   node3 

5、初始化所有节点的mon服务   ceph-deploy   mon   create-initial
(3、4、5 主机名解析必须对，而且是在  ceph-cluster 目录下执行的)

常见问题：
 安装包出现错误的时候检查yum源，rpm -q  ceph-mon    ceph-osd 有没装
 初始化节点mon服务出现错误，把每一台机器的mon服务停了，再重新初始化一次

6、创建OSD

1）、准备磁盘分区 ，将/dev/vdb 分为gpt模式，vdb1和vdb2个一半储存空间，用来存储服务器的日志journal盘
parted  /dev/vdb   mklabel   gpt       //将设备格式化为gpt模式
parted  /dev/vdb   mkpart   primary    1M   50%
parted  /dev/vdb   mkpart   primary    50%   100%    //主分区模式，一人一半空间

chown   ceph:ceph   /dev/vdb1
chown  ceph:ceph   /dev/vdb2
以上操作要在三台存储虚拟机器上操作

由于系统重起后，设备/dev/vdb？的所属者和所属组会改变，所以我们需要通过udev来闯进规则，永久的保存设备的所属关系
进入  cd /etc/udev/rules.d/ 目录先，查看
[root@node1 rules.d]# udevadm info -a -p /block/vdb/vdb1 |less
创建规则文件：
vim /etc/udev/rules.d/100-ceph.rules 
ACTION=="add", KERNEL=="vdb?",SUBSYSTEM=="block", OWNER="ceph",GROUP="ceph"
重启服务：
[root@node2 rules.d]# systemctl restart systemd-udev-trigger.service
查看：
ls -l /dev/vdb1
brw-rw----. 1 ceph disk 252, 17 8月  11 14:54 /dev/vdb1


2）、初始化清空磁盘数据 （仅在一个节点上操作就行了）
ceph-deploy   disk   zap   node1:vdc   node1:vdd
ceph_deploy   disk   zap   node2:vdc   node2:vdd
ceph_deploy   disk   zap   node3:vdc   node3:vdd

3)、创建OSD存储空间  （仅在一个节点上操作就行了）
ceph-deploy  osd   create node1:vdc:/dev/vdb1 node1:vdd:/dev/vdb2
ceph-deploy  osd   create node2:vdc:/dev/vdb1 node3:vdd:/dev/vdb2
ceph-deploy  osd   create node3:vdc:/dev/vdb1 node3:vdd:/dev/vdb2
//创建osd存储设备，vdc为集群提供存储空间，vdb提供journal 日志
//一个存储设备对应一个日志设备，

4）、测试    ceph   -s    （健康检查，ok 则可以继续往下做）
根据报错信息出现的常见问题： 
1、 NTP时间同步问题，需要将chrony服务重启，并且将ceph服务重启
2、/dev/vdb{1,2} 所属关系的所有者和所属组都应该是 ceph
3、yum源的问题，可检查 rpm -q  ceph-mon和 ceph-osd 两个软件包，以及配置文件
4、查看每一台的磁盘信息和挂载信息，确定是哪台有问题，可重新初始化清空再创建osd存储空间
5、具体情况，具体分析


三、创建ceph块存储

1、创建存储池： 查看：    ceph  osd   lspools 

2、创建镜像、查看镜像
创建：
rbd  create  镜像名   --image-feature   layering   --size    10G   (不指定存储池默认在 0 rbd， )
 或者  rbd   create   储存池/镜像名   --image-feature   layering  --size  10G
查看： rbd  ls   、  rbd   info   镜像名

3、动态调整镜像的大小
缩小：rbd   resize   --size   7G  镜像名   --allow-shrink
扩容：rbd  resize   --size   15G  镜像名


4、通过KRBD访问：   通过内核加载ceph块存储

1）、集群内将镜像映射为本地磁盘
 rbd  map   镜像名   ---》  lsblk    ---》  格式化   ---》 挂载

2）、客户端通过KRBD访问
 安装 ceph-common 软件包 
 拷贝配置文件 /etc/ceph/ceph.conf   (否则不知道集群在哪)
 拷贝连接密钥 （否则无连接权限）

3）、 映射     rbd  map   镜像名   ---》 lsblk  ---》 查看  rbd  showmapped    ---> 格式化，挂载
   取消映射   rbd  unmap  /dev/rbd/rbd/镜像名
           rbd  unmap  /dev/rbd/{poolname}/{imagename}                   
     
4）、挂载后在挂载点创建文件  ，然后在服务端创建镜像快照
   rbd  snap   create   镜像名   --snap  快照名
 查看镜像快照 ：  rbd  snap   ls   镜像名

5）客户端将文件删除，然后 umount ，然后还原快照
client：umount   /dev/rbd0     /mnt
node1：  rbd  snap  rollback   镜像名   --snap  快照名
client： mount   /dev/rdb0   /mnt
查看，发现文件回来了  

无法还原的常见原因：
客户端和服务端都需要挂载，这个需要检查
先创建文件再创建快照，才能吧文件还原回来
先取消挂载，再回滚，否则无法挂载，要是顺序错了，再回滚一次就ok了
创建和回滚在服务端和客户端都可以，

6）、创建克隆快照

  rbd  clone   镜像名  --snap  快照名  克隆名   --image-feature  layering  //用快照克隆一个新的镜像
克隆失败的可能原因：  （快照未保护 ，只有被保护的快照才能进行克隆）
  保护快照：  rbd  snap  protect  镜像名  --snap  快照名
 删除快照：   rbd  snap  rm   镜像名   --snap   快照名    （被保护了无法删除）
删除多个快照：  rbd snap purge 快照名/快照名  
 取消保护：  rbd  snap  unprotect  镜像名   --snap  快照名

查看克隆镜像和父镜像的关系：  rbd  info   克隆镜像名
## 克隆镜像很多数据都来自快照链   
## 如果希望克隆镜像可以独立工作，需要将父快照中的数据全部拷贝一份，但比较耗费时间
##创建克隆好了一个依赖于父镜像快照的RBD镜像后，为了让这个克隆的RBD镜像独立于父镜像，我们需要将父镜像的信息合并flattern到子镜像，一旦合并完成，RBD镜像和它的父镜像就不会存在任何关系了。使用flatten合并
   rbd  flatten   镜像名
克隆镜像就成了独立的镜像了

7）、删除快照与镜像：
删除快照： rbd  snap   rm   镜像名   --snap  快照名
删除镜像： rbd  rm   镜像

无法删除快照的常见原因：
快照被保护，需要unprotect，再删除
打错快照名，可用 rbd  snap  ls  镜像名 查看一下要删除的快照名

无法删除镜像的常见原因：
需要将镜像对应的快照都删掉
需要将挂载点卸下来，服务端和客户端

8)、删除集群节点，删除集群节点数据
  rbd  purge  node1  node2  node3
  rbd  purgedata  node1  node2   node3
  ---》删除后重新创建目录，并且进入目录，开始创建集群



Ceph数据存储过程

在Ceph存储系统中，数据存储分三个映射过程，首先要将用户要操作的file，映射为RADOS能够处理的object。就是简单的按照object的size对file进行切分，相当于RAID中的条带化过程。接着把Object映射到PG，在file被映射为一个或多个object之后，就需要将每个object独立地映射到一个PG中去。第三次映射就是将作为object的逻辑组织单元的PG映射到数据的实际存储单元OSD。

文件存入时，首先把File切分为RADOS层面的Object，每个Object一般为2MB或4MB(大小可设置)。每个Object通过哈希算法映射到唯一的PG。每个PG通过Crush算法映射到实际存储单元OSD，PG和OSD间是多对多的映射关系。OSD在物理上可划分到多个故障域中，故障域可以跨机柜和服务器，通过策略配置使PG的不同副本位于不同的故障域中


ceph架构的理解：
http://www.360doc.com/content/16/0520/03/6902273_560595312.shtml      
https://blog.csdn.net/swingwang/article/details/52280748

